\newpage
\fancyhead[R]{18 November 2021}
\subsection{The Curse of Dimensionality}

When $X$ is a random vector in $\R^d$, we use kernel estimators of the form
\begin{equation}
  \hat{f}_X\pa{x} = \frac{1}{nh^d} \sum_{i=1}^n K_d\pa{\frac{X_i-x}{h}},
\end{equation}
where $K_d$ is a $d$-dimensional kernel, when the smoothness is ``homogeneous'', else we can use different bandwidths along different dimensions.

\begin{example}
  \begin{equation}
    K_d\pa{\frac{X_i -x}{h}} = \prod_{k=1}^d K\pa{\frac{X_{ki} - x_k}{h} },
  \end{equation}
  where $K$ is a usual one-dimensional kernel.
\end{example}

In the TD, we will see that we can obtain rates for the $\MISE$ of the order $n^{-\frac{2\beta}{2\beta +d}}$, where $\beta$ is the Sobolev smoothness, see definition in TD. As $d\rightarrow \infty$, we do not have any rate of convergence. The higher $d$, the smaller the rate of convergence.

The fact that the rate depends on $d$ is called the ``curse of dimensionality''.

\subsection{Other Estimators in \texorpdfstring{$d$}{d} dimensions}
\begin{enumerate}
  \item \textbf{Modified kernel estimators,} e.g.~the $k$-th nearest neighbor estimator
  \begin{equation}
    \hat{f}_X \frac{1}{n\hat{h}\pa{x}^d} \sum_{i=1}^k K_d \pa{\frac{X_i-x}{\hat{h}\pa{x}}},
  \end{equation}
  where $\hat{x}$ is the distance between $x$ and the $k$-th nearest $X_i$ of $x$.
\end{enumerate}

\begin{remark}
  There are multiple choices for the distance, e.g. $d^2\pa{x,y}= x^T \Sigma y$, where $\Sigma$ is symmetric positive definite.
\end{remark}

% \newpage
% \fancyhead[L]{Nonparametric MLE}
% \section{Nonparametric Maximum Likelihood Estimation}

\begin{enumerate}
\setcounter{enumi}{1}
\item \textbf{Nonparametric MLE.}
The Likelihood is $\prod_{i=1}^n f_X\pa{X_i}$. It does not make sense to maximize over all $f_X$ because we get $\infty$ by taking $\hat{f}_X = \frac{1}{n}\sum_{i=1}^n \frac{1}{h}\one_{\bra{X_i -\frac{h}{2}, X_i + \frac{h}{2}}}$ and letting $h\rightarrow \infty$ the limit is not a density.
So we restrict over classes, e.g. $\mathcal P_w(\beta,L)$. There are only few remarkable examples when the NPMLE has a simple form, one is $\mathcal P=\set{\mathrm{monotone\;decreasing\;densities\;over\;}[0,\infty ) }$.

in this case there is an explicit solution called the Grenander estimator.

\item One can consider \textbf{penalized versions of the NPMLE}.

\cite{geman_hwang} considered $\hat{f}_X$ which is solution to
\begin{equation}
  \min_{f} \set{-\frac{1}{n} \sum_{i=1}^n \log f\pa{X_i} + \lambda \norm{f^{\pa{\ell}}}_2^2}.
\end{equation} where $\lambda>0$ is called the \textit{smoothing parameter}, $\lambda$ can be chosen by cross-validation in practice.

\cite{good_gaskins} proposed $\hat{f}_X$ which is solution to \footnote{Note that $\int_\R \frac{\pa{f'}^2}{f}\pa{x} dx$ occurs to be the Fisher information.}:
\begin{equation}
\min_{f\geq 0} \set{-\frac{1}{n} \sum_{i=1}^n  \log f\pa{X_i}  + \lambda_1 \int_\R f\pa{x} dx+ \lambda _2 \int_\R \frac{\pa{f'}^2}{f}\pa{x}dx}
\end{equation}

Take $u= \sqrt{f}$, the problem becomes
\begin{equation}
  \min_{u\geq 0} \set{-\frac{2}{n}\sum_{i=1}^n \log u \pa{X_i} + \lambda_1\norm{u}_2^2 + 4\lambda_2\norm{u'}_2^2 }.
\end{equation}

\item \textbf{Orthogonal series\footnote{Generalizations are called sieves (in Econometrics) or dictionaries in machine-learning.}.}
Let $f_X\in L^2\pa{\bra{0,1}^d}$, where $L^2\pa{\bra{0,1}^d}$ can be proven to be a \textit{separable Hilbert space} when endowed with the inner product
\begin{equation}
\angs{f,g}=\int_\R f(x)g(x) dx.
\end{equation}
We write
\begin{equation}
\norm{f}_2^2 = \angs{f,f}.
\end{equation}

{\color{blue}Some properties are comparable to $\R^d$ with $\angs{x,y} = x^T y$.} As a separable space, $L^2\pa{[0,1]^d}$ has a countable basis $\pa{e_j}_{j=1}^\infty$, which is a sequence of functions in $L^2\pa{[0,1]^d}$ such that for all
\begin{equation}
\angs{e_j,e_k} =\delta_{jk} = \begin{cases}
1, &j=k,\\
0, &\text{else},
\end{cases}
\end{equation}
and for all $f\in L^2\pa{\bra{0,1}^d}$,
\begin{equation}
f=\lim_{k\rightarrow\infty} \sum_{j=1}^k \angs{f,e_j} e_j.
\end{equation}
{\color{blue}Think of $\R^d$, where $\pa{e_j}_{j=1}^d$ is a basis for $e_j =\pa{0,\ldots,0,1,0,\ldots,0}$ the $j$-th unit vector. Then $\angs{e_j,e_k} = e_j^T e_k = \delta_{jk}$, and for $x\in \R^d$,
\begin{equation}
  x = \sum_{j=1}^d x_j e_j = \sum_{j=1}^d x^T e_j e_j = \sum_{j=1}^d \angs{x,e_j} e_j.
\end{equation}}

Given $\pa{e_j}_{j=1}^\infty$ a basis, for all $f\in L^2\pa{[0,1]^d}$, \begin{equation}
\norm{f}^2_2=\sum_{j=1}^\infty \angs{f,e_j}^2.
\end{equation}
This is a version of the Pythagorean theorem. {\color{blue}In $\mathbb{R}^d$,
\begin{equation}
\norm{x}^2_2 = \sum_{j=1}^d x_j^2 = \sum_{j=1}^d \angs{x,e_j}^2.
\end{equation}}

Back to our goal to estimate $f_X = \lim_{k\rightarrow \infty}\sum_{j=1}^\infty \angs{f,e_j} e_j$. For some $T\in \N$, consider $f_X^T \defeq \sum_{j=1}^T \angs{f,e_j} e_j$. The idea is to estimate this cut-off sum instead of the limit expression for $f_X$. We have
\begin{equation}
c_j \defeq \angs{f_X,e_j}=\int_{[0,1]^d}f_X(x)e_j(x)dx = \E\bra{e_j(X)},
\end{equation}
so that an unbiased estimator is
\begin{equation}
\hat{c}_j = \frac{1}{n}\sum_{i=1}^n e_j\pa{X_i}.
\end{equation}

Thus, a candidate estimator for $f_X$ is
\begin{equation}
  \hat{f}_X^T = \sum_{j=1}^T \hat{c}_j e_j,
\end{equation} where
\begin{equation}
\E\bra{\hat{f}_X^T}=\sum_{j=1}^T c_j e_j = f_X^T.
\end{equation}

It is possible to write
\begin{equation}
  \hat{f}_X^T=\frac{1}{n} \sum_{i=1}^n \underbrace{\sum_{j=1}^T e_j(X_i)e_j(x)}_{q_T\pa{X_i,x}},
\end{equation}
where $q_T\pa{X_i,x}$ plays the role of a kernel and $T$ plays the same role as $\frac{1}{h}$.

On $L^2\pa{[0,1]^d}$ we can use bases for which $e_j=f_{j_1}\cdots f_{j_d}$ where $\pa{f_k}_{k=1}^\infty$ is a basis of $L^2\pa{[0,1]}$ and $\pa{j_1,...,j_d}$ plays the role of the index $j$\footnote{Note that there exists a bijection $\N^d\rightarrow \N$.}.

For example, $f_{k}\pa{x} = \sqrt{2}\sin\pa{\pi k x}$ is a basis of $L^2\pa{[0,1]}$.

This gives
\begin{equation}
  e_{j_1,\ldots,j_d} \pa{x} = 2^{\frac{d}{2}} \prod_{k=1}^d \sin\pa{\pi j_k x_k}.
\end{equation}
One can check that this defines an orthogonal system \textbf{(Exercise)}.

We define
\begin{equation}
\begin{split}
W\pa{\beta, L} = &\left\{ \vphantom{\sum_{j_d=1}^\infty}f: \bra{0,1}^d\rightarrow \R \text{ with coefficients } c_{{j_1},\ldots,{j_d}}\text{ w.r.t. } \pa{f_{j_1}, \ldots, f_{j_d}} \right.  \\
&\quad\quad \left. \text{such that } \sum_{j_1=1}^\infty \sum_{j_2=1}^\infty\cdots \sum_{j_d=1}^\infty c_{j_1,...,j_d}^2\pa{j_1^2+...+j_d^2}^\beta\leq L^2 \right\}
\end{split}
\end{equation}

{\color{olive}In $L^2\pa{\R^d}$, an analogous condition (with Fourier transform instead of Fourier series) would be:
\begin{equation}
\int_{\R^d} \abs{\mathcal F[f]\pa{w_1,...,w_d}}^2\pa{\abs{w_1}^2+...+\abs{w_d}^2}^\beta dw \leq L^2.
\end{equation}}

Note the usual bias-variance decomposition of the mean-squared error,
\begin{equation}
  \E\bra{\norm{\hat f_X^T - f_X}^2_2} = \underbrace{\norm{f_X^T - f_X }^2_2}_{b^2=\text{Bias}^2} + \underbrace{\E\bra{\norm{\hat{f}_X^T - f_X^T}_2^2}}_{\sigma^2}.
\end{equation}
Then
\begin{equation}
\begin{split}
  b^2 &= \sum_{j_1 = T+1}^\infty \cdots \sum_{j_d = T+1}^\infty c^2_{j_1,\ldots, j_d} \\
  &\leq \sum_{j_1 = T+1}^\infty \cdots \sum_{j_d = T+1}^\infty c^2_{j_1,\ldots, j_d} \pa{\pa{\frac{j_1}{T+1}}^2 + \cdots + \pa{\frac{j_1}{T+1}}^2}^\beta \\
  &= \pa{\frac{1}{T+1}}^{2\beta } \sum_{j_1 = T+1}^\infty \cdots \sum_{j_d = T+1}^\infty c^2_{j_1,\ldots, j_d} \pa{j_1^2 + \cdots + j_d^2}^\beta \\
  &\leq \pa{\frac{1}{T+1}}^{2\beta} L^2.
\end{split}
\end{equation}

Note that $\norm{f_{j_1}\cdots f_{j_d}}_2^2 = \norm{f_{j_1}}_2^2 \cdots \norm{f_{j_d}}_2^2$, which are all $=1$. Then,
\begin{equation}
\begin{split}
  \sigma^2 &= \E\bra{ \norm{\hat{f}_X^T - f_X^T}_2^2}\\
   &= \E\bra{\sum_{j_1=1}^T \cdots \sum_{j_d=1}^T \pa{\hat{c}_{j_1,\ldots,j_d} - c_{j_1,\ldots, j_d}}^2\norm{f_{j_1}\cdots f_{j_d}}^2_2 }\\
   &= \E\bra{\sum_{j_1=1}^T \cdots \sum_{j_d=1}^T \pa{\hat{c}_{j_1,\ldots,j_d} - c_{j_1,\ldots, j_d}}^2} \\
  &= \sum_{j_1=1}^T \cdots \sum_{j_d=1}^T \Var\pa{\hat c_{j_1,...,j_d}}   \\
  &\leq \sum_{j_1=1}^T \cdots \sum_{j_d=1}^T \frac{2^d}{n^2}\\
  &\leq \frac{\pa{2 \pa{T+1} }^d }{n}
\end{split}
\end{equation}
since
\begin{equation}
\begin{split}
  \Var\pa{\hat c_{j_1,...,j_d}} &= \frac{1}{n^2} \sum_{i=1}^n \Var \pa{f_{j_1} \cdots f_{j_d} \pa{X_i}} \\
  &\leq \frac{1}{n^2} \sum_{i=1}^n \E \bra{f_{j_1}^2 \cdots f_{j_d}^2 \pa{X_i}}\\
  &\leq \frac{2^d}{n^2},
  \end{split}
\end{equation}
where we used $f_{j_k}^2 \leq 2$ for our particular basis $f_{j_k}\pa{x} = \sqrt{2}\sin\pa{\pi j_k x}$.

Remember $\sigma^2 \leq \frac{1}{nh^d}$ for kernel estimators.

This gives an upper bound of the order of $n^{-\frac{2\beta}{2\beta +d }}$ if $T = \left\lfloor n^{\frac{1}{2\beta + d}}\right\rfloor$.
\end{enumerate}
